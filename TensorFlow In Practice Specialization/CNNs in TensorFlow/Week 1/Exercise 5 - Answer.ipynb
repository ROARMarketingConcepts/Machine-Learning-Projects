{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Exercise 5 - Answer.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/Exercises/Exercise%205%20-%20Real%20World%20Scenarios/Exercise%205%20-%20Answer.ipynb","timestamp":1595717421455}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"zX4Kg8DUTKWO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595717513793,"user_tz":240,"elapsed":1216,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dn-6c02VmqiN","colab":{},"executionInfo":{"status":"ok","timestamp":1595717515023,"user_tz":240,"elapsed":2439,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}}},"source":["import os\n","import zipfile\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from shutil import copyfile"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3sd9dQWa23aj","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1595717572836,"user_tz":240,"elapsed":60250,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}},"outputId":"00bbdc2f-efef-4c11-83b6-b8aecaea5ea1"},"source":["# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n","# And right click on the 'Download Manually' link to get a new URL to the dataset\n","\n","# Note: This is a very large dataset and will take time to download\n","\n","!wget --no-check-certificate \\\n","    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n","    -O \"/tmp/cats-and-dogs.zip\"\n","\n","local_zip = '/tmp/cats-and-dogs.zip'\n","zip_ref   = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2020-07-25 22:51:55--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n","Resolving download.microsoft.com (download.microsoft.com)... 92.123.124.137, 2a02:26f0:fe00:1af::e59, 2a02:26f0:fe00:1a1::e59\n","Connecting to download.microsoft.com (download.microsoft.com)|92.123.124.137|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 824894548 (787M) [application/octet-stream]\n","Saving to: ‘/tmp/cats-and-dogs.zip’\n","\n","/tmp/cats-and-dogs. 100%[===================>] 786.68M  16.8MB/s    in 47s     \n","\n","2020-07-25 22:52:43 (16.7 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DM851ZmN28J3","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595717572838,"user_tz":240,"elapsed":60249,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}},"outputId":"9366ef7f-fc60-44f2-c239-5ff7e00e2907"},"source":["print(len(os.listdir('/tmp/PetImages/Cat/')))\n","print(len(os.listdir('/tmp/PetImages/Dog/')))\n","\n","# Expected Output:\n","# 12501\n","# 12501"],"execution_count":4,"outputs":[{"output_type":"stream","text":["12501\n","12501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F-QkLjxpmyK2","colab":{},"executionInfo":{"status":"ok","timestamp":1595717572839,"user_tz":240,"elapsed":60248,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}}},"source":["try:\n","    os.mkdir('/tmp/cats-v-dogs')\n","    os.mkdir('/tmp/cats-v-dogs/training')\n","    os.mkdir('/tmp/cats-v-dogs/testing')\n","    os.mkdir('/tmp/cats-v-dogs/training/cats')\n","    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n","    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n","    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n","except OSError:\n","    pass"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zvSODo0f9LaU","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595717578965,"user_tz":240,"elapsed":66371,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}},"outputId":"4f516b37-7adb-405b-d46d-91e20b8349b1"},"source":["def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","    files = []\n","    for filename in os.listdir(SOURCE):\n","        file = SOURCE + filename\n","        if os.path.getsize(file) > 0:\n","            files.append(filename)\n","        else:\n","            print(filename + \" is zero length, so ignoring.\")\n","\n","    training_length = int(len(files) * SPLIT_SIZE)\n","    testing_length = int(len(files) - training_length)\n","    shuffled_set = random.sample(files, len(files))\n","    training_set = shuffled_set[0:training_length]\n","    testing_set = shuffled_set[-testing_length:]\n","\n","    for filename in training_set:\n","        this_file = SOURCE + filename\n","        destination = TRAINING + filename\n","        copyfile(this_file, destination)\n","\n","    for filename in testing_set:\n","        this_file = SOURCE + filename\n","        destination = TESTING + filename\n","        copyfile(this_file, destination)\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":6,"outputs":[{"output_type":"stream","text":["666.jpg is zero length, so ignoring.\n","11702.jpg is zero length, so ignoring.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hwHXFhVG3786","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1595717578966,"user_tz":240,"elapsed":66370,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}},"outputId":"dacc9316-438d-4095-f1a3-73b0101af0da"},"source":["print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","\n","# Expected output:\n","# 11250\n","# 11250\n","# 1250\n","# 1250"],"execution_count":7,"outputs":[{"output_type":"stream","text":["11250\n","11250\n","1250\n","1250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-BQrav4anTmj","colab":{},"executionInfo":{"status":"ok","timestamp":1595717594081,"user_tz":240,"elapsed":81482,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}}},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fQrZfVgz4j2g","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595717594297,"user_tz":240,"elapsed":81696,"user":{"displayName":"Ken Wood","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7sa_QK499GnOYPrJBbSk4KUijT44dLCxY6tx8BA=s64","userId":"01072404293886020231"}},"outputId":"0c2b750d-1220-423c-fbfa-839b01a5d663"},"source":["TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n","train_datagen = ImageDataGenerator(rescale=1.0/255.)\n","train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n","                                                    batch_size=100,\n","                                                    class_mode='binary',\n","                                                    target_size=(150, 150))\n","\n","VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n","validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n","                                                              batch_size=100,\n","                                                              class_mode='binary',\n","                                                              target_size=(150, 150))\n","\n","# Expected Output:\n","# Found 22498 images belonging to 2 classes.\n","# Found 2500 images belonging to 2 classes."],"execution_count":9,"outputs":[{"output_type":"stream","text":["Found 22498 images belonging to 2 classes.\n","Found 2500 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5qE1G6JB4fMn","colab":{"base_uri":"https://localhost:8080/","height":581},"outputId":"9bc9a441-2e0a-4802-d497-a3f16872d678"},"source":["# Note that this may take some time.\n","history = model.fit(train_generator,\n","                              epochs=50,\n","                              verbose=1,\n","                              validation_data=validation_generator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n"," 24/225 [==>...........................] - ETA: 55s - loss: 1.0645 - accuracy: 0.5213"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"],"name":"stderr"},{"output_type":"stream","text":["225/225 [==============================] - 74s 330ms/step - loss: 0.6753 - accuracy: 0.6353 - val_loss: 0.5778 - val_accuracy: 0.6944\n","Epoch 2/50\n","225/225 [==============================] - 74s 328ms/step - loss: 0.5061 - accuracy: 0.7509 - val_loss: 0.4537 - val_accuracy: 0.7892\n","Epoch 3/50\n","225/225 [==============================] - 73s 326ms/step - loss: 0.4316 - accuracy: 0.7997 - val_loss: 0.4603 - val_accuracy: 0.7912\n","Epoch 4/50\n","225/225 [==============================] - 73s 326ms/step - loss: 0.3707 - accuracy: 0.8350 - val_loss: 0.4177 - val_accuracy: 0.8172\n","Epoch 5/50\n","225/225 [==============================] - 73s 326ms/step - loss: 0.3009 - accuracy: 0.8703 - val_loss: 0.4228 - val_accuracy: 0.8208\n","Epoch 6/50\n","225/225 [==============================] - 73s 326ms/step - loss: 0.2306 - accuracy: 0.9054 - val_loss: 0.4221 - val_accuracy: 0.8292\n","Epoch 7/50\n","225/225 [==============================] - 73s 325ms/step - loss: 0.1556 - accuracy: 0.9384 - val_loss: 0.4808 - val_accuracy: 0.8216\n","Epoch 8/50\n","135/225 [=================>............] - ETA: 26s - loss: 0.1039 - accuracy: 0.9622"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MWZrJN4-65RC","colab":{}},"source":["%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","plt.figure()\n","\n","\n","# Desired output. Charts with training and validation metrics. No crash :)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LqL6FYUrtXpf","colab":{}},"source":["# Here's a codeblock just for fun. You should be able to upload an image here \n","# and have it classified without crashing\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(150, 150))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]}]}